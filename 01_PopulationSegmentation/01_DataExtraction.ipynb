{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fab6fa",
   "metadata": {},
   "source": [
    "<b>Project</b>: Population segmentation and transition probability estimation using data on health and health-related social service needs from the US Health and Retirement Study <br>\n",
    "<b>Project section</b>: Population segmentation <br>\n",
    "\n",
    "<b>Version</b>: ???Python 3.68.01 <br>\n",
    "\n",
    "<b>File name</b>: 01_Extraction.ipynb <br>\n",
    "<b>Data required</b>: Health and Retirement Study (HRS) datasets: RAND HRS Longitudinal File 2018, Cross-Wave Tracker File, 2018 RAND HRS Fat File, 2016 RAND HRS Fat File, 2014 RAND HRS Fat File, 2012 RAND HRS Fat File, 2010 RAND HRS Fat File, 2008 RAND HRS Fat File, 2006 RAND HRS Fat File <br>\n",
    "<b>Outcome</b>: Extracts necessary data from various Health and Retirement Study (HRS) datasets and prepares a dataset for segmentation <br>\n",
    "\n",
    "<b>Author</b>: Lize Duminy<br>\n",
    "<b>Date</b>: 2023.03.19 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2952b3e",
   "metadata": {},
   "source": [
    "# Instructions for use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41496dea",
   "metadata": {},
   "source": [
    "This script requires raw data to be downloaded from the Health and Retirement Study (HRS) platform for researchers. The daw data required for segmentation are public datasets, accessable to all persons with a registered HRS user account. \n",
    "\n",
    "1. If you do not already have a user account on the HRS platform for researchers, follow the instruction in [User account creation userform](https://hrsdata.isr.umich.edu/user/register) to create a new user account. \n",
    "2. Log into the [HRS platform](https://hrsdata.isr.umich.edu/user/login).\n",
    "3. Navigate to each of the __data product pages__ listed in the table below and download the specified __zip folders__. If any of the zip folders listed are no longer availible (since the folders are incrementally replaced with newer versions), download the STATA version of the availible file and update the name of the folder in the preparation section of the code in __1.2 USER INPUT REQUIRED: Change filename if any of the zip folders were updated with newer versions__. For further documentation related to each data product, see the linked __documentation__.   \n",
    "5. Extract all zip folders into a folder designated for raw data on your system.\n",
    "6. Replace the filepath of the variable __global_path__ in the code below (currently _C:/Users/LizeDuminy/data/HRS/_) with the filepath of your designated folder for raw data in __1.1. USER INPUT REQUIRED: Replace this filepath with the filepath of your designated folder for raw data__. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef76b3",
   "metadata": {},
   "source": [
    "| Data product page | Zip folder name | Documentation |\n",
    "| -: | :-: | :- |\n",
    "| [RAND HRS Longitudinal File 2018](https://hrsdata.isr.umich.edu/data-products/rand-hrs-longitudinal-file-2018) | __randhrs1992_2018v2_STATA.zip__ | [randhrs1992_2018v2.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/1658948491/randhrs1992_2018v2.pdf) |\n",
    "| [Cross-Wave Tracker File](https://hrsdata.isr.umich.edu/data-products/cross-wave-tracker-file) | __trk2020v2.zip__ | [README_trk20.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/data-descriptions/1671578103/trk20.pdf) |\n",
    "| [2018 RAND HRS Fat File](https://hrsdata.isr.umich.edu/data-products/2018-rand-hrs-fat-file) | __h18f2a_STATA.zip__ | [README_fat18.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/1658866817/README_fat18.pdf) |\n",
    "| [2016 RAND HRS Fat File](https://hrsdata.isr.umich.edu/data-products/2016-rand-hrs-fat-file) | __h16f2b_STATA.zip__ | [README_fat16.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/1615838140/README_fat16.pdf) |\n",
    "| [2014 RAND HRS Fat File](https://hrsdata.isr.umich.edu/data-products/2014-rand-hrs-fat-file) | __h14f2b_STATA.zip__ | [README_fat14.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/1615583715/README_fat14.pdf) |\n",
    "| [2012 RAND HRS Fat File](https://hrsdata.isr.umich.edu/data-products/2012-rand-hrs-fat-file) | __h12f3a_STATA.zip__ | [README_fat12.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/1615837583/README_fat12.pdf) |\n",
    "| [2010 RAND HRS Fat File](https://hrsdata.isr.umich.edu/data-products/2010-rand-hrs-fat-file) | __hd10f6a_STATA.zip__ | [README_fat10.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/1658943550/README_fat10.pdf) |\n",
    "| [2008 RAND HRS Fat File](https://hrsdata.isr.umich.edu/data-products/2008-rand-hrs-fat-file) | __h08f3a_STATA.zip__ | [README_fat08.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/README_fat08.pdf) |\n",
    "| [2006 RAND HRS Fat File](https://hrsdata.isr.umich.edu/data-products/2006-rand-hrs-fat-file) | __h06f4a_STATA.zip__ | [README_fat06.pdf](https://hrsdata.isr.umich.edu/sites/default/files/documentation/other/1658871121/README_fat06.pdf) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f25624",
   "metadata": {},
   "source": [
    "# 1. Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b294ab1",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e857f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pyreadstat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbbd81",
   "metadata": {},
   "source": [
    "### 1.1. USER INPUT REQUIRED: Replace this filepath with the filepath of your designated folder for raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31368210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File path for daw data\n",
    "global_path=os.path.join(\"C:/Users/LizeDuminy/data/HRS/raw_data/\")\n",
    "\n",
    "#File path for script output\n",
    "global_path2=os.path.join(\"C:/Users/LizeDuminy/data/HRS/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f20044",
   "metadata": {},
   "source": [
    "### 1.2. USER INPUT REQUIRED: Change filename if any of the zip folders were updated with newer versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6298eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAND HRS Longitudinal File 2018\n",
    "path_randhrs = os.path.join(global_path , \"randhrs1992_2018v2_STATA/randhrs1992_2018v2.dta\")\n",
    "\n",
    "#Cross-Wave Tracker File\n",
    "path_trk = os.path.join(global_path , \"trk2020v2/trk2020tr_r.dta\")\n",
    "\n",
    "#RAND HRS Fat File\n",
    "path_hrsCore = {\n",
    "    14: \"h18f2a_STATA/h18f2a.dta\",\n",
    "    13: \"h16f2b_STATA/h16f2b.dta\",\n",
    "    12: \"h14f2b_STATA/h14f2b.dta\", \n",
    "    11: \"h12f3a_STATA/h12f3a.dta\", \n",
    "    10: \"hd10f6a_STATA/hd10f6a.dta\", \n",
    "    9:  \"h08f3a_STATA/h08f3a.dta\", \n",
    "    8:  \"h06f4a_STATA/h06f4a.dta\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a772cf4",
   "metadata": {},
   "source": [
    "# 2. RAND HRS Longitudinal File 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8ab93",
   "metadata": {},
   "source": [
    "This section extracts and transforms the __RAND HRS Longitudinal File 2018__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d858d",
   "metadata": {},
   "source": [
    "### Load RAND HRS data as \"df_randhrs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5988a",
   "metadata": {},
   "source": [
    "#### Define file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77767979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_randhrs = os.path.join(global_path , \"randhrs1992_2018v2_STATA/randhrs1992_2018v2.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44520ca5",
   "metadata": {},
   "source": [
    "#### Load longitudinal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09ac9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_randhrs = pd.read_stata(path_randhrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bdec2",
   "metadata": {},
   "source": [
    "#### Define columns that have a wave-specific pre-fix on individual level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfbd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    #chronic conditions\n",
    "    'hibp',    #High blood pressure\n",
    "    'diab',    #Diabetes\n",
    "    'cancr',   #Cancer (excl. skin cancer)\n",
    "    'lung',    #Chronic lung disease such as chronic bronchitis or emphysema\n",
    "    'heart',   #Heart attack, coronary heart disease, angina, congestive heart failure, or other heart problems\n",
    "    'strok',   #Stroke or cerebral vascular disease\n",
    "    'arthr',   #Arthritis or rheumatism\n",
    "    \n",
    "    #self-reported health\n",
    "    'shlt',\n",
    "    \n",
    "    #Depression\n",
    "    'cesd',    #CES-D score\n",
    "    \n",
    "    #Frailty\n",
    "    'weight',  #Self-reported weight\n",
    "    'bmi',     #Self-reported BMI\n",
    "    'height',  #Self-reported height\n",
    "    'grp',     #hand grip strength maximum measurement\n",
    "    'timwlk',  #timed walk test time\n",
    "    'ltactx',  #Freq light phys activ\n",
    "    'mdactx',  #Freq moderate phys activ\n",
    "    'vgactx',  #Freq vigorous phys activ\n",
    "    'effort',  #CESD: Everything an effort\n",
    "    'going',    #CESD: Could not get going\n",
    "\n",
    "    ###Complicating Features\n",
    "    'adl6h', # bathe, dress, and eat, getting in/out of bed and walking across a room and using the toilet\n",
    "    'iadl5a', #using the phone, managing money, and taking medications, shopping for groceries and preparing hot meals\n",
    "    'homcar', #in the last two years, has any medically-trained person come to your home to help you? MEDICALLY-TRAINED PERSONS INCLUDE PROFESSIONAL NURSES, VISITING NURSE'S AIDES, PHYSICAL OR OCCUPATIONAL THERAPISTS, CHEMOTHERAPISTS, RESPIRATORY OXYGEN THERAPISTS, AND HOSPICE CAREGIVERS\n",
    "    'psyche', #\n",
    "    'inhptn', #Total number of helpers ever helped\n",
    "    'hlppdtn',  #HLPPDTN: Total number of helpers paid to help\n",
    "    'hsptim', #RwHSPTIM is the reported number of hospital stays\n",
    "\n",
    "    #### Utilization\n",
    "    #'hsptim', #RwHSPTIM is the reported number of hospital stays\n",
    "    'hspnit',    #Hospitl nghts, prv 12 mos    \n",
    "    'nrsnit',    #nights in nurs home, prv 2 yrs\n",
    "    'doctim',    #number of doctor visits, prv 12 mos\n",
    "    'homcar',    #home hlth care, prv 2 yrs\n",
    "    'outpt',     #outpatient surgry, prev 2 yrs\n",
    "    'spcfac',    #Spec hlth facilty, prv 2 yrs\n",
    "    'oopmd',     #Mexp Amt:Out of pkt med exp, prv 2 yrs\n",
    "    \n",
    "    #### Insurance\n",
    "    'higov',      #R was covered by Gov plan Categ\n",
    "    'govmr',      #R had Gov plan-Medicare Categ\n",
    "    'govmd',      #R has Gov plan-Medicaid Categ\n",
    "    'govva',      #R had Gov plan-CHAMPUS/CHAMPVA Categ\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd8c50",
   "metadata": {},
   "source": [
    "#### Define prefix per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd10406",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_prefix = {\n",
    "     8:'r8',\n",
    "     9:'r9',\n",
    "    10:'r10',\n",
    "    11:'r11',\n",
    "    12:'r12',\n",
    "    13:'r13',\n",
    "    14:'r14',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ba27b",
   "metadata": {},
   "source": [
    "#### Define columns with a wave-specific pre-fix, for which a lag-variable is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768175b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_wave_variables = {\n",
    "    'weight',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e1f26",
   "metadata": {},
   "source": [
    "#### Define lag prefix per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b249ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_wave_prefix = {\n",
    "     8:'r7', \n",
    "     9:'r8',\n",
    "    10:'r9',\n",
    "    11:'r10',\n",
    "    12:'r11',\n",
    "    13:'r12',\n",
    "    14:'r13',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e6165",
   "metadata": {},
   "source": [
    "#### Define function to extract and transform wave-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18752565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_randhrs(wave):\n",
    "    \n",
    "    #Start with the full RAND HRS dataset containing data from 1992 to 2018\n",
    "    df = pd.DataFrame(df_randhrs)\n",
    "    \n",
    "    #Define relevant variables\n",
    "    selected_variables = ['hhidpn'] + ['ragender'] + ['racohbyr'] + [wave_prefix[wave] + variable for variable in variables] + [prev_wave_prefix[wave] + variable for variable in prev_wave_variables] #+ [household_wave_prefix[wave] + variable for variable in variables_hh] \n",
    "    \n",
    "    #Reduce columns to include wave-specific columns required for global impressions segmentation\n",
    "    df = df[df.columns.intersection(selected_variables)]\n",
    "    \n",
    "    #Reformat hhidpn variable\n",
    "    df.hhidpn = df.hhidpn.astype(int) #remove decimal point for hhidpn \n",
    "    df.hhidpn = df.hhidpn.astype(object) #convert from numeric to a string\n",
    "    df.hhidpn = df.hhidpn.astype(str).str.zfill(9) #fill the left with zero's to make hhidpn 9 digits\n",
    "    \n",
    "    #Rename lag variables\n",
    "    lhs_prev = [prev_wave_prefix[wave] + variable for variable in prev_wave_variables] #current name of variable\n",
    "    rhs_prev = ['prev_' + variable for variable in prev_wave_variables] #desired name of variable\n",
    "    fin_var_prev = len(prev_wave_variables)\n",
    "    for x in range(0, fin_var_prev):\n",
    "        df.rename(columns = {lhs_prev[x]:rhs_prev[x]}, inplace = True) #rename columns\n",
    "    \n",
    "    #Rename gender\n",
    "    df.rename(columns = {'ragender':'gender'}, inplace = True) #rename columns\n",
    "    \n",
    "    #Remove wave specific prefixes on individual level\n",
    "    lhs = [wave_prefix[wave] + variable for variable in variables] #current name of variable\n",
    "    rhs = [variable for variable in variables] #desired name of variable\n",
    "    fin_var = len(variables)\n",
    "    for x in range(0, fin_var):\n",
    "        df.rename(columns = {lhs[x]:rhs[x]}, inplace = True) #rename columns\n",
    "       \n",
    "    #Add wave indicator\n",
    "    df['wave'] = wave\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a7f19",
   "metadata": {},
   "source": [
    "#### Extract and transform dataset per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf408579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each wave should have the same number of columns extracted\n",
      "41  columns extracted from wave 8\n",
      "41  columns extracted from wave 9\n",
      "41  columns extracted from wave 10\n",
      "41  columns extracted from wave 11\n",
      "41  columns extracted from wave 12\n",
      "41  columns extracted from wave 13\n",
      "41  columns extracted from wave 14\n"
     ]
    }
   ],
   "source": [
    "df_8_a = extract_transform_randhrs(8)\n",
    "df_9_a = extract_transform_randhrs(9)\n",
    "df_10_a = extract_transform_randhrs(10)\n",
    "df_11_a = extract_transform_randhrs(11)\n",
    "df_12_a = extract_transform_randhrs(12)\n",
    "df_13_a = extract_transform_randhrs(13)\n",
    "df_14_a = extract_transform_randhrs(14)\n",
    "\n",
    "print(\"Each wave should have the same number of columns extracted\")\n",
    "print(len(df_8_a.columns), \" columns extracted from wave 8\")\n",
    "print(len(df_9_a.columns), \" columns extracted from wave 9\")\n",
    "print(len(df_10_a.columns), \" columns extracted from wave 10\")\n",
    "print(len(df_11_a.columns), \" columns extracted from wave 11\")\n",
    "print(len(df_12_a.columns), \" columns extracted from wave 12\")\n",
    "print(len(df_13_a.columns), \" columns extracted from wave 13\")\n",
    "print(len(df_14_a.columns), \" columns extracted from wave 14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f7c35",
   "metadata": {},
   "source": [
    "#### Identify columns missing from first wave (wave 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "938378f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')  <- columns missing from wave 8\n"
     ]
    }
   ],
   "source": [
    "print(df_8_a.columns.symmetric_difference(df_9_a.columns), \n",
    "      \" <- columns missing from wave 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c81185",
   "metadata": {},
   "source": [
    "#### Identify columns missing from first wave (wave 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a73fd3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')  <- columns missing from wave 14\n"
     ]
    }
   ],
   "source": [
    "print(df_14_a.columns.symmetric_difference(df_9_a.columns), \n",
    "      \" <- columns missing from wave 14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb415194",
   "metadata": {},
   "source": [
    "### Define \"df_randhrs_2\" as stacked dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a979ff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each wave should have same amount of entries\n",
      "8     42233\n",
      "9     42233\n",
      "10    42233\n",
      "11    42233\n",
      "12    42233\n",
      "13    42233\n",
      "14    42233\n",
      "Name: wave, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_randhrs_2 = [df_8_a, \n",
    "               df_9_a, \n",
    "               df_10_a, \n",
    "               df_11_a, \n",
    "               df_12_a, \n",
    "               df_13_a, \n",
    "               df_14_a]\n",
    "df_randhrs_2 = pd.concat(df_randhrs_2)\n",
    "\n",
    "print(\"Each wave should have same amount of entries\")\n",
    "print(df_randhrs_2[\"wave\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8772d",
   "metadata": {},
   "source": [
    "#  3. Cross-Wave Tracker File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff345e58",
   "metadata": {},
   "source": [
    "This section extracts and transforms the __Cross-Wave Tracker File__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa778d02",
   "metadata": {},
   "source": [
    "### Load Cross-Wave Tracker File data as \"df_trk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d25439",
   "metadata": {},
   "source": [
    "#### Define file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3eda5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_trk = os.path.join(global_path , \"trk2020v2/trk2020tr_r.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928e89f",
   "metadata": {},
   "source": [
    "#### Load tracker data as \"df_trk\" and format column headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e2c41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trk = pd.read_stata(path_trk)\n",
    "df_trk.columns= df_trk.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a6ea9",
   "metadata": {},
   "source": [
    "#### Define columns that have a wave-specific pre-fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85d88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    'iwmonth', #Month that interview took place\n",
    "    'iwyear', #Year that interview took place\n",
    "    'iwtype', #Wave X Whether Interviewed in the Wave\n",
    "    'alive',   # interview status per wave\n",
    "    'pmwgtr', #final respondent weight for wave's physical measures subsample\n",
    "    'couple', #married or living with a partner as if married), the variable xCOUPLE is equal to 1. Otherwise, it is 5.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014e42d",
   "metadata": {},
   "source": [
    "#### Define prefix per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b00a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_prefix = {\n",
    "     8:'k',\n",
    "     9:'l',\n",
    "    10:'m',\n",
    "    11:'n',\n",
    "    12:'o',\n",
    "    13:'p',\n",
    "    14:'q',\n",
    "    15:'r',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1735e",
   "metadata": {},
   "source": [
    "#### Define columns that to not have a wave-specific pre-fix and convert set to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db83242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_fixed_variables = {\n",
    "    'hhidpn',             #unique id\n",
    "    'birthmo',            #year of death\n",
    "    'birthyr',            #month of death\n",
    "    'knowndeceasedmo',    #best information about death date for deceased respondents\n",
    "    'knowndeceasedyr',    #best information about death date for deceased respondents\n",
    "    'lastalivemo',        #month on which we last knew the respondent was alive\n",
    "    'lastaliveyr',        #year on which we last knew the respondent was alive\n",
    "    'phymsr04',           #sample indicator for participation in physical measures section\n",
    "    'eftfassign',         #The variable EFTFASSIGN holds the respondents’ permanent assignment for enhanced face-to-face rotation from 2006 onward.\n",
    "}\n",
    "\n",
    "selected_fixed_variables = list(selected_fixed_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd15751",
   "metadata": {},
   "source": [
    "#### Define function to extract and transform wave-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c1908dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_trk(wave):\n",
    "    \n",
    "    #Start with the full tracker dataset containing data from 1992 to 2018\n",
    "    df = pd.DataFrame(df_trk)\n",
    "    \n",
    "    #Construct unique ID by combining household and personal ID\n",
    "    df[\"hhidpn\"] = df[\"hhid\"] + df[\"pn\"]\n",
    "    \n",
    "    #Define relevant variables with wave-specific prefixes \n",
    "    selected_variables = ['hhidpn'] + [wave_prefix[wave] + variable for variable in variables]\n",
    "\n",
    "    #Reduce columns to include wave-specific columns required for global impressions segmentation\n",
    "    df_wave_specific = df[df.columns.intersection(selected_variables)]\n",
    "    df_wave_specific = pd.DataFrame(df_wave_specific)\n",
    "    \n",
    "    #Remove wave specific prefixes\n",
    "    df_wave_specific.columns = df_wave_specific.columns.str[1:]\n",
    "    df_wave_specific.rename(columns={\"hidpn\": \"hhidpn\"}, inplace = True)\n",
    "    \n",
    "    #Reduce columns to include non-wave-specific columns required for global impressions segmentation\n",
    "    df_non_wave_specific = df[df.columns.intersection(selected_fixed_variables)]\n",
    "    df_non_wave_specific = pd.DataFrame(df_non_wave_specific)\n",
    "    \n",
    "    #Merge dynamic and static variables\n",
    "    df = pd.merge(df_wave_specific, df_non_wave_specific, how=\"outer\", on=\"hhidpn\")\n",
    "    \n",
    "    #Add wave indicator\n",
    "    df['wave'] = wave\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a1120",
   "metadata": {},
   "source": [
    "#### Extract and transform dataset per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cefe35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each wave should have the same number of columns extracted (exept wave 15)\n",
      "16  columns extracted from wave 8\n",
      "16  columns extracted from wave 9\n",
      "16  columns extracted from wave 10\n",
      "16  columns extracted from wave 11\n",
      "16  columns extracted from wave 12\n",
      "16  columns extracted from wave 13\n",
      "16  columns extracted from wave 14\n",
      "15  columns extracted from wave 15\n",
      "The weights are not yet available for the preliminary data released in wave 15\n"
     ]
    }
   ],
   "source": [
    "df_8_b = extract_transform_trk(8)\n",
    "df_9_b = extract_transform_trk(9)\n",
    "df_10_b = extract_transform_trk(10)\n",
    "df_11_b = extract_transform_trk(11)\n",
    "df_12_b = extract_transform_trk(12)\n",
    "df_13_b = extract_transform_trk(13)\n",
    "df_14_b = extract_transform_trk(14)\n",
    "df_15_b = extract_transform_trk(15)\n",
    "\n",
    "print(\"Each wave should have the same number of columns extracted (exept wave 15)\")\n",
    "print(len(df_8_b.columns), \" columns extracted from wave 8\")\n",
    "print(len(df_9_b.columns), \" columns extracted from wave 9\")\n",
    "print(len(df_10_b.columns), \" columns extracted from wave 10\")\n",
    "print(len(df_11_b.columns), \" columns extracted from wave 11\")\n",
    "print(len(df_12_b.columns), \" columns extracted from wave 12\")\n",
    "print(len(df_13_b.columns), \" columns extracted from wave 13\")\n",
    "print(len(df_14_b.columns), \" columns extracted from wave 14\")\n",
    "print(len(df_15_b.columns), \" columns extracted from wave 15\") \n",
    "print(\"The weights are not yet available for the preliminary data released in wave 15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f66de0a",
   "metadata": {},
   "source": [
    "#### Identify columns missing from first wave (wave 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2290115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')  <- columns missing from wave 8\n"
     ]
    }
   ],
   "source": [
    "print(df_8_b.columns.symmetric_difference(df_9_b.columns), \n",
    "      \" <- columns missing from wave 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38390c2e",
   "metadata": {},
   "source": [
    "#### Identify columns missing from second last wave (wave 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e837edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')  <- columns missing from wave 14\n"
     ]
    }
   ],
   "source": [
    "print(df_14_b.columns.symmetric_difference(df_9_b.columns), \n",
    "      \" <- columns missing from wave 14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb361b",
   "metadata": {},
   "source": [
    "#### Identify columns missing from last wave (wave 15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8d4f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pmwgtr'], dtype='object')  <- columns missing from wave 15\n",
      "Anticipate pmwgtr to be missing\n"
     ]
    }
   ],
   "source": [
    "print(df_15_b.columns.symmetric_difference(df_9_b.columns), \n",
    "      \" <- columns missing from wave 15\")\n",
    "print(\"Anticipate pmwgtr to be missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c68cf0",
   "metadata": {},
   "source": [
    "### Define \"df_trk_2\" as stacked dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6618928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each wave should have same amount of entries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8     43559\n",
       "9     43559\n",
       "10    43559\n",
       "11    43559\n",
       "12    43559\n",
       "13    43559\n",
       "14    43559\n",
       "15    43559\n",
       "Name: wave, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trk_2 = [df_8_b, \n",
    "            df_9_b, \n",
    "            df_10_b, \n",
    "            df_11_b, \n",
    "            df_12_b, \n",
    "            df_13_b, \n",
    "            df_14_b,\n",
    "            df_15_b]\n",
    "\n",
    "df_trk_2 = pd.concat(df_trk_2)\n",
    "\n",
    "print(\"Each wave should have same amount of entries\")\n",
    "df_trk_2[\"wave\"].value_counts().sort_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c047cfd",
   "metadata": {},
   "source": [
    "### Define \"df_trk_3\" as additional tracking file of persons who died in wave 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bafd8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trk_3 = pd.DataFrame(df_15_b)\n",
    "df_trk_3 = df_trk_3[df_trk_3.alive==5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a24e4",
   "metadata": {},
   "source": [
    "# 4. RAND HRS Fat Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712eb3ce",
   "metadata": {},
   "source": [
    "Extract and transform __2018 RAND HRS Fat File__, __2016 RAND HRS Fat File__, __2014 RAND HRS Fat File__, __2012 RAND HRS Fat File__, __2010 RAND HRS Fat File__, __2008 RAND HRS Fat File__, __2006 RAND HRS Fat File__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ef96d",
   "metadata": {},
   "source": [
    "In the RAND HRS dataset, the cognition variables and self-reported taking of prescription medications are taken directly from the HRS imputations of cognitive functioning. These imputations are calculated\n",
    "only for the final release data, meaning that there are no cognitive variables released for Wave 14 yet. We therefore extract all cognition variables from the HRS Core dataset to ensure consistency across waves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0712ed",
   "metadata": {},
   "source": [
    "#### Define wave-specific file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f300d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value: path\n",
    "#wave_path = {\n",
    "#    14: \"h18f2a_STATA/h18f2a.dta\",\n",
    "#    13: \"h16f2b_STATA/h16f2b.dta\",\n",
    "#    12: \"h14f2b_STATA/h14f2b.dta\", \n",
    "#    11: \"h12f3a_STATA/h12f3a.dta\", \n",
    "#    10: \"hd10f6a_STATA/hd10f6a.dta\", \n",
    "#    9:  \"h08f3a_STATA/h08f3a.dta\", \n",
    "#    8:  \"h06f4a_STATA/h06f4a.dta\",\n",
    "#    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb19a9c",
   "metadata": {},
   "source": [
    "#### Define prefix per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81e997ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value: prefix\n",
    "wave_prefix = {\n",
    "    14: \"Q\",\n",
    "    13: \"P\",\n",
    "    12: \"O\",\n",
    "    11: \"N\",\n",
    "    10: \"M\",\n",
    "    9: \"L\",\n",
    "    8: \"K\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffaba8b",
   "metadata": {},
   "source": [
    "#### Define required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a3fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    'D174',    #Immediate word recall\n",
    "    'D184',    #Delayed word recall\n",
    "    'D142',    #Serial 7 test - 1\n",
    "    'D143',    #Serial 7 test - 2\n",
    "    'D144',    #Serial 7 test - 3\n",
    "    'D145',    #Serial 7 test - 4\n",
    "    'D146',    #Serial 7 test - 5\n",
    "    'D155',    #Object naming test: scissors\n",
    "    'D156',    #Object naming test: cactus\n",
    "    'G097',    #Suppose in the future, you needed help with basic personal care activities like eating or dressing. Do you have relatives or friends (besides your [husband/wife/partner]) who would be willing and able to help you over a long period of time?\n",
    "    \n",
    "    #Reported taking medication for\n",
    "    'C006',    # high blood pressure or hypertension\n",
    "    'C011',    # treat or control your [diabetes/blood sugar]\n",
    "    'C032',    # treatment for your lung condition\n",
    "    'C037',    # taking or carrying medication for your heart problem\n",
    "    'C042',    # taking or carrying medication because of your heart attack\n",
    "    'C046',    # taking or carrying medications because of angina or chest pain\n",
    "    'C050',    # taking or carrying any medication for congestive heart failure\n",
    "    'C282',    # In order to regulate your heart rhythm are you now taking any medication\n",
    "    'C060',    # taking any medications because of your stroke or its complications \n",
    "    #'C287',    # Over-the-counter pain medications include such things as Advil, Aleve, Tylenol, aspirin or similar medications. In the past three months have you taken any over-the-counter pain medications for the treatment of pain?\n",
    "    'N360',    # To help lower your cholesterol?\n",
    "    'N361',    # For pain in your joints or muscles?\n",
    "    'N362',    # For asthma or allergies or other breathing problems?\n",
    "    'N363',    # For stomach problems?\n",
    "    'N364',    # To help you sleep?\n",
    "    'N365',    # To help relieve anxiety or depression?\n",
    "    'N283',    # Do you regularly take prescription medications other than aspirin to thin your blood or to prevent blood clots?\n",
    "    \n",
    "    #Out of pocket payments\n",
    "    'N106',    # hospital care\n",
    "    'N119',    # nursing home care\n",
    "    'N139',    # out of hospital surgery\n",
    "    'N156',    # doctor visits\n",
    "    'N168',    # dental care\n",
    "    'N180',    # prescriptions\n",
    "    'N194',    # in-home health care\n",
    "    'N328',    # hospice care (exit interview only)\n",
    "    'N239',    # other services\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae379c",
   "metadata": {},
   "source": [
    "#### Define function to extract and transform wave-specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fecf22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_transform_HSR_Core(wave):\n",
    "    \n",
    "    #Load wave-specific dataset\n",
    "    df = pd.read_stata(global_path + path_hrsCore[wave])\n",
    "    \n",
    "    #Define as dataframe\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    #Harmonize heading cases across waves\n",
    "    df.columns = df.columns.str.upper() # convert all headings to uppercase (necessary for wave: 6,8 and 9)\n",
    "    \n",
    "    #Define relevant variables with wave-specific prefixes\n",
    "    selected_variables = [\"HHIDPN\"]+[wave_prefix[wave]+ variable for variable in variables] # select all relevant variables\n",
    "    \n",
    "    #Reduce columns to include wave-specific columns required for global impressions segmentation\n",
    "    df = df[df.columns.intersection(selected_variables)] # access the selected variables\n",
    "    \n",
    "    #Remove wave specific prefixes\n",
    "    df.columns = df.columns.str[1:]\n",
    "    df.rename(columns={\"HIDPN\": \"hhidpn\"}, inplace = True)\n",
    "    \n",
    "    #Reformat hhidpn variable\n",
    "    df.hhidpn = df.hhidpn.astype(int) #remove decimal point for hhidpn \n",
    "    df.hhidpn = df.hhidpn.astype(object) #convert from numeric to a string\n",
    "    df.hhidpn = df.hhidpn.astype(str).str.zfill(9) #fill the left with zero's to make hhidpn 9 digits\n",
    "        \n",
    "    #Add wave indicator\n",
    "    df['wave'] = wave\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9da57867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each wave should have the same number of columns extracted\n",
      "34  columns extracted from wave 8\n",
      "34  columns extracted from wave 9\n",
      "34  columns extracted from wave 10\n",
      "36  columns extracted from wave 11\n",
      "36  columns extracted from wave 12\n",
      "36  columns extracted from wave 13\n",
      "36  columns extracted from wave 14\n"
     ]
    }
   ],
   "source": [
    "df_8_c = extract_and_transform_HSR_Core(8)\n",
    "df_9_c = extract_and_transform_HSR_Core(9)\n",
    "df_10_c = extract_and_transform_HSR_Core(10)\n",
    "df_11_c = extract_and_transform_HSR_Core(11)\n",
    "df_12_c = extract_and_transform_HSR_Core(12)\n",
    "df_13_c = extract_and_transform_HSR_Core(13)\n",
    "df_14_c = extract_and_transform_HSR_Core(14)\n",
    "\n",
    "print(\"Each wave should have the same number of columns extracted\")\n",
    "print(len(df_8_c.columns), \" columns extracted from wave 8\")\n",
    "print(len(df_9_c.columns), \" columns extracted from wave 9\")\n",
    "print(len(df_10_c.columns), \" columns extracted from wave 10\")\n",
    "print(len(df_11_c.columns), \" columns extracted from wave 11\")\n",
    "print(len(df_12_c.columns), \" columns extracted from wave 12\")\n",
    "print(len(df_13_c.columns), \" columns extracted from wave 13\")\n",
    "print(len(df_14_c.columns), \" columns extracted from wave 14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5a7cf",
   "metadata": {},
   "source": [
    "### Define \"df_hrsCore\" as stacked dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e732283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each wave should have a different amount of entries since the rows reflects the actual number of participants in each wave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8     18469\n",
       "9     17217\n",
       "10    22034\n",
       "11    20554\n",
       "12    18747\n",
       "13    20912\n",
       "14    17146\n",
       "Name: wave, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hrsCore = [df_8_c, \n",
    "              df_9_c, \n",
    "              df_10_c, \n",
    "              df_11_c, \n",
    "              df_12_c, \n",
    "              df_13_c, \n",
    "              df_14_c]\n",
    "\n",
    "df_hrsCore = pd.concat(df_hrsCore)\n",
    "\n",
    "print(\"Each wave should have a different amount of entries since the rows reflects the actual number of participants in each wave\")\n",
    "df_hrsCore[\"wave\"].value_counts().sort_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d682a",
   "metadata": {},
   "source": [
    "# 5. Merge dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d208e",
   "metadata": {},
   "source": [
    "This section merges df_randhrs_2, df_trk_2, and df_hrsCore into a single dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78a48b",
   "metadata": {},
   "source": [
    "#### Define testing variables to evaluate merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bb678a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_randhrs_2[\"a\"] = 1\n",
    "df_trk_2[\"b\"] = 1\n",
    "df_hrsCore[\"c\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ca8d1",
   "metadata": {},
   "source": [
    "#### Merge df_randhrs_2 with df_hsrCore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ef73a",
   "metadata": {},
   "source": [
    "Perform a left merge on df_randhrs_2 and df_hrsCore. Keep all entries from df_randhrs_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32eac37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset contains  295631  rows from df_randhrs_2\n",
      "Merged dataset contains  135079  rows from df_hsrCore\n",
      "Rows merged correctly\n",
      "Columns merged correctly\n"
     ]
    }
   ],
   "source": [
    "data0 = pd.merge(df_randhrs_2, df_hrsCore, how=\"left\", on=[\"hhidpn\", \"wave\"])\n",
    "        \n",
    "#Test whether all entries of the dataset consists of data from both df_randhrs_2 and df_trk_2\n",
    "a = int(data0['a'].sum())\n",
    "c = int(data0['c'].sum())\n",
    "print(\"Merged dataset contains \", a, \" rows from df_randhrs_2\")\n",
    "print(\"Merged dataset contains \", c, \" rows from df_hsrCore\")\n",
    "\n",
    "#Test whether (1) all rows from df_randhrs_2 are in the merged dataset\n",
    "#AND (2) there is at least one row from df_hsrCore in the merged dataset\n",
    "if (a == len(df_randhrs_2)) & (c > 0):\n",
    "    print(\"Rows merged correctly\")\n",
    "else:\n",
    "    print(\"Rows not merged correctly\")\n",
    "\n",
    "#Check if columns have merged correctly\n",
    "cols_tot = [col for col in df_hrsCore.columns] + [col for col in df_randhrs_2.columns]\n",
    "cols_tot = np.array(cols_tot)\n",
    "cols_tot = len(np.unique(cols_tot))\n",
    "cols_merged = len(df_hrsCore.columns) + len(df_randhrs_2.columns) - 2\n",
    "\n",
    "#Test if all columns from both datasets are present in the merged dataset\n",
    "if cols_tot == cols_merged:\n",
    "    print(\"Columns merged correctly\")\n",
    "else:\n",
    "    print(\"Columns not merged correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88818585",
   "metadata": {},
   "source": [
    "#### Merge data0 with df_trk_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc479c",
   "metadata": {},
   "source": [
    "Perform a left merge on data0 and df_trk_2. Keep all entries from data0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02d8076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_randhrs_2: 295631\n",
      "Number of rows in df_trk_2: 295617.0\n",
      "Number of rows in merged dataframe 295631\n",
      "Rows merged correctly\n",
      "Columns merged correctly\n"
     ]
    }
   ],
   "source": [
    "#Merge data0 with df_hsrCore. Keep all entries  \n",
    "data = pd.merge(data0, df_trk_2, how=\"left\", on=[\"hhidpn\", \"wave\"])\n",
    "\n",
    "#Merge dataframes\n",
    "#data = pd.merge(df_randhrs_2, df_trk_2, how=\"inner\", on=[\"hhidpn\", \"wave\"])\n",
    "\n",
    "#Test whether all entries of the dataset consists of data from both df_randhrs_2 and df_trk_2\n",
    "a = int(data[\"a\"].value_counts().sort_index())\n",
    "b = int(data[\"b\"].value_counts().sort_index())\n",
    "\n",
    "#Confirm that the same number of entries are present across each wave\n",
    "print(\"Number of rows in df_randhrs_2:\", data[\"a\"].sum())\n",
    "print(\"Number of rows in df_trk_2:\", data[\"b\"].sum())\n",
    "print(\"Number of rows in merged dataframe\", len(data))\n",
    "\n",
    "#Confirm that the merge did not lose information\n",
    "if (max(a, b) == len(data)) & (min(a, b) > 0):\n",
    "    print(\"Rows merged correctly\")\n",
    "else:\n",
    "    print(\"Rows not merged correctly\")    \n",
    "\n",
    "#Check if columns have merged correctly\n",
    "cols_tot = [col for col in df_hrsCore.columns] + [col for col in df_randhrs_2.columns] + [col for col in df_trk_2.columns]\n",
    "cols_tot = np.array(cols_tot)\n",
    "cols_tot = len(np.unique(cols_tot))\n",
    "cols_merged = len(df_hrsCore.columns) + len(df_randhrs_2.columns) - 2 + len(df_trk_2.columns) - 2\n",
    "\n",
    "#Test if all columns from both datasets are present in the merged dataset\n",
    "if cols_tot == cols_merged:\n",
    "    print(\"Columns merged correctly\")\n",
    "else:\n",
    "    print(\"Columns not merged correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4671ac",
   "metadata": {},
   "source": [
    "# 6. Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd431a",
   "metadata": {},
   "source": [
    "#### Evaluate starting number of entries per wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "431e4c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries per wave:\n",
      "8     42233\n",
      "9     42233\n",
      "10    42233\n",
      "11    42233\n",
      "12    42233\n",
      "13    42233\n",
      "14    42233\n",
      "Name: wave, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_data_0 = data[\"wave\"].value_counts().sort_index()\n",
    "print(\"Number of entries per wave:\")\n",
    "print(n_data_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d07ad3",
   "metadata": {},
   "source": [
    "#### Remove entries for whom living status is not known for given wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b65b7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries per wave:\n",
      "8     28974\n",
      "9     28964\n",
      "10    35483\n",
      "11    35228\n",
      "12    35016\n",
      "13    39374\n",
      "14    39471\n",
      "Name: wave, dtype: int64\n",
      " \n",
      "Number of entries dropped per wave:\n",
      "8     13259\n",
      "9     13269\n",
      "10     6750\n",
      "11     7005\n",
      "12     7217\n",
      "13     2859\n",
      "14     2762\n",
      "Name: wave, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data.dropna(subset = [\"alive\"], inplace=True) \n",
    "    \n",
    "n_data_1 = data[\"wave\"].value_counts().sort_index()\n",
    "n_dropped_1 = n_data_0 - n_data_1\n",
    "print(\"Number of entries per wave:\")\n",
    "print(n_data_1)\n",
    "print(\" \")\n",
    "print(\"Number of entries dropped per wave:\")\n",
    "print(n_dropped_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fa686",
   "metadata": {},
   "source": [
    "#### Remove entries if their death was recorded in a previous wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54d2cb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries per wave:\n",
      "8     22049\n",
      "9     20659\n",
      "10    25880\n",
      "11    24016\n",
      "12    22630\n",
      "13    25645\n",
      "14    24261\n",
      "Name: wave, dtype: int64\n",
      " \n",
      "Number of entries dropped per wave:\n",
      "8      6925\n",
      "9      8305\n",
      "10     9603\n",
      "11    11212\n",
      "12    12386\n",
      "13    13729\n",
      "14    15210\n",
      "Name: wave, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = data[data.alive != 6]\n",
    "\n",
    "n_data_2 = data[\"wave\"].value_counts().sort_index()\n",
    "n_dropped_2 = n_data_1 - n_data_2\n",
    "print(\"Number of entries per wave:\")\n",
    "print(n_data_2)\n",
    "print(\" \")\n",
    "print(\"Number of entries dropped per wave:\")\n",
    "print(n_dropped_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435b983",
   "metadata": {},
   "source": [
    "#### Remove entries if no contact was made in the given wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3423ea1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries per wave:\n",
      "8     21745\n",
      "9     20486\n",
      "10    25467\n",
      "11    23602\n",
      "12    22056\n",
      "13    22462\n",
      "14    18369\n",
      "Name: wave, dtype: int64\n",
      " \n",
      "Number of entries dropped per wave:\n",
      "8      304\n",
      "9      173\n",
      "10     413\n",
      "11     414\n",
      "12     574\n",
      "13    3183\n",
      "14    5892\n",
      "Name: wave, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = data[data.alive != 2]\n",
    "\n",
    "n_data_3 = data[\"wave\"].value_counts().sort_index()\n",
    "n_dropped_3 = n_data_2 - n_data_3\n",
    "print(\"Number of entries per wave:\")\n",
    "print(n_data_3)\n",
    "print(\" \")\n",
    "print(\"Number of entries dropped per wave:\")\n",
    "print(n_dropped_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1dad9f",
   "metadata": {},
   "source": [
    "#### Remove respondents who are recorded as dead in the first interview wave that they participated in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1167c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries per wave:\n",
      "8     20361\n",
      "9     20473\n",
      "10    25461\n",
      "11    23602\n",
      "12    22056\n",
      "13    22462\n",
      "14    18369\n",
      "Name: wave, dtype: int64\n",
      "Number of entries dropped:\n",
      "1403\n"
     ]
    }
   ],
   "source": [
    "#Drop all respondents that are reported as dead in wave 8\n",
    "n_data_5 = len(data)\n",
    "               \n",
    "#sort individuals per id, per wave\n",
    "data_alive = pd.DataFrame(data)\n",
    "data_alive = data_alive.sort_values(['hhidpn', 'wave'])\n",
    "\n",
    "#isolate first interview per ID\n",
    "data_alive = data_alive.groupby('hhidpn').first().reset_index()\n",
    "data_alive = pd.DataFrame(data_alive)\n",
    "\n",
    "#drop all entries that are not alive\n",
    "data_alive = data_alive[data_alive.alive==1]\n",
    "\n",
    "#define series as all IDs in data\n",
    "s = pd.Series(data.hhidpn)\n",
    "\n",
    "#identify which of the IDs have a live first observation\n",
    "s = s.isin(pd.Series(data_alive.hhidpn))\n",
    "\n",
    "#restrict the data to only individuals that have at least one live observation\n",
    "data = data[s]\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "n_data_6 = len(data)\n",
    "\n",
    "#Evaluate entries removed per wave\n",
    "n_dropped_6 = n_data_5 - n_data_6\n",
    "print(\"Number of entries per wave:\")\n",
    "print(data[\"wave\"].value_counts().sort_index())\n",
    "print(\"Number of entries dropped:\")\n",
    "print(n_dropped_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b10b277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    144648\n",
       "5.0      8136\n",
       "Name: alive, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.alive.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2087a",
   "metadata": {},
   "source": [
    "#### Remove entries from respondents younger than 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ce5b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries per wave:\n",
      "8     19753\n",
      "9     20053\n",
      "10    24414\n",
      "11    22863\n",
      "12    21554\n",
      "13    21679\n",
      "14    17908\n",
      "Name: wave, dtype: int64\n",
      " \n",
      "Number of entries dropped per wave:\n",
      "8      608\n",
      "9      420\n",
      "10    1047\n",
      "11     739\n",
      "12     502\n",
      "13     783\n",
      "14     461\n",
      "Name: wave, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#assign random birth month if birth month is not known\n",
    "data['randmo'] = [np.random.randint(1, 12) for i in range(0, len(data))] #random.randint(1,12,size=(0,len(data))) #data.birthmo\n",
    "data['birthmo_2'] = data.birthmo\n",
    "data.loc[(data.birthmo == 0), 'birthmo_2'] = data.randmo\n",
    "\n",
    "#calculate time at birth\n",
    "data[\"birthtime\"] = ((data.birthmo_2-1)/12+data.birthyr)\n",
    "\n",
    "#calculate time of interview\n",
    "data[\"iwtime\"] = ((data.iwmonth-1)/12+data.iwyear)\n",
    "\n",
    "#isolate wave and time of interview \n",
    "data_iw_m = data[['wave', 'iwtime']]\n",
    "data_iw_m = pd.DataFrame(data_iw_m)\n",
    "\n",
    "#Calculate the mean interview date per wave\n",
    "data_iw_m = data_iw_m.groupby('wave')[['iwtime']].mean()\n",
    "data_iw_m = data_iw_m.reset_index()\n",
    "\n",
    "#rename variable\n",
    "data_iw_m.rename(columns = {'iwtime':'iwtime_mean'}, inplace = True)\n",
    "\n",
    "#Insert mean interview date per wave to data\n",
    "data = pd.merge(data, data_iw_m, how=\"left\", on=[\"wave\"])\n",
    "data.loc[pd.isnull(data.iwtime), 'iwtime'] = data.iwtime_mean\n",
    "\n",
    "#Calculate age at time of interview\n",
    "data['age'] = data.iwtime - data.birthtime\n",
    "\n",
    "\n",
    "n_data_7 = data[\"wave\"].value_counts().sort_index()\n",
    "#drop all entries that are not alive\n",
    "data = data[data.age>=50]\n",
    "\n",
    "n_data_8 = data[\"wave\"].value_counts().sort_index()\n",
    "n_dropped_8 = n_data_7 - n_data_8\n",
    "print(\"Number of entries per wave:\")\n",
    "print(n_data_8)\n",
    "print(\" \")\n",
    "print(\"Number of entries dropped per wave:\")\n",
    "print(n_dropped_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc2f17",
   "metadata": {},
   "source": [
    "#### Remove entries where respondent did not complete at least one physical measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e101317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries per wave:\n",
      "8      7167\n",
      "9      7705\n",
      "10    10065\n",
      "11     9141\n",
      "12     8815\n",
      "13     9346\n",
      "14     8137\n",
      "Name: wave, dtype: int64\n",
      "Number of entries dropped per wave:\n",
      "8     12586\n",
      "9     12348\n",
      "10    14349\n",
      "11    13722\n",
      "12    12739\n",
      "13    12333\n",
      "14     9771\n",
      "Name: wave, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_data_9 = data[\"wave\"].value_counts().sort_index()\n",
    "\n",
    "#Remove individuals who did not participate in the physical measures section of the survey\n",
    "data = data[(data.pmwgtr>0)|(data.alive==5)]\n",
    "\n",
    "\n",
    "#Evaluate entries removed per wave\n",
    "n_data_10 = data[\"wave\"].value_counts().sort_index()\n",
    "n_dropped_10 = n_data_9 - n_data_10\n",
    "print(\"Number of entries per wave:\")\n",
    "print(n_data_10)\n",
    "print(\"Number of entries dropped per wave:\")\n",
    "print(n_dropped_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94238c",
   "metadata": {},
   "source": [
    "# 7. Extraction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b3cf619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations per wave:\n",
      "8      7167\n",
      "9      7705\n",
      "10    10065\n",
      "11     9141\n",
      "12     8815\n",
      "13     9346\n",
      "14     8137\n",
      "Name: wave, dtype: int64\n",
      " \n",
      "60376  <- total number of observations in dataset\n",
      "52260  <- total enhanced face-to-face interviews\n",
      "8116  <- total participants deceased\n",
      "27352  <- number of individuals in study\n"
     ]
    }
   ],
   "source": [
    "print(\"Observations per wave:\")\n",
    "print(data.wave.value_counts().sort_index())\n",
    "print(\" \")\n",
    "\n",
    "n_total = len(data)\n",
    "print(n_total, \" <- total number of observations in dataset\")\n",
    "\n",
    "n_interview = len(data[data.alive!=5])\n",
    "print(n_interview, \" <- total enhanced face-to-face interviews\")\n",
    "\n",
    "n_deceased = len(data[data.alive==5])\n",
    "print(n_deceased, \" <- total participants deceased\")\n",
    "\n",
    "n_uniqueID = len(data.hhidpn.unique())\n",
    "print(n_uniqueID, \" <- number of individuals in study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599de699",
   "metadata": {},
   "source": [
    "# 8. Export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38c621",
   "metadata": {},
   "source": [
    "### Write \"dataHRS.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b7a57",
   "metadata": {},
   "source": [
    "Write dataset to be used in 02_Global_Impressions_Segmentation.ipynb and 03_Complicating_Features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b3f5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataHRS = os.path.join(global_path2, \"dataHRS.csv\")\n",
    "\n",
    "data.to_csv(filepath_dataHRS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120c957",
   "metadata": {},
   "source": [
    "### Write \"df_trk_2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebec0e",
   "metadata": {},
   "source": [
    "Write to dataset to be used in 04_SurvivalAnalysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ccba9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(global_path2, \"df_trk_2.csv\")\n",
    "\n",
    "df_trk_2.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91559a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
